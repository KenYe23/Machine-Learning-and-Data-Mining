---
title: "hw1"
author: "Ken Ye"
date: "`r Sys.Date()`"
output: pdf_document
---

# Question 5

This question involves the use of multiple linear regression on the Auto data
set.

```{r}
library(ISLR)
data(Auto)
attach(Auto)
```

(a) Produce a scatterplot matrix which includes all of the variables in the data
    set.

```{r}
pairs(Auto)
```

(b) Compute the matrix of correlations between the variables using the function
    cor(). You will need to exclude the name variable, which is qualitative.

```{r}
cor(Auto[,-9])
```

(c) Use the lm() function to perform a multiple linear regression with mpg as
    the response and all other variables except name as the predictors. Use the
    summary() function to print the results.

```{r}
mlr <- lm(mpg ~ . -name, data = Auto)
summary(mlr)
```

Comment on the output. For instance:

i\. Is there a relationship between the predictors and the response?

There does seem to be a relationship between the predictors and the response,
indicated by the large F-statistic and the near-zero p-value. In addition, the
adjusted R-squared is 0.8182, which is quite high and indicates that 81.82% of
the model variability is explained by the predictors.

ii\. Which predictors appear to have a statistically significant relationship to
the response?

These predictors appear to have a statistically significant relationship to the
response as their p-values are all \< 0.05: displacement, weight, year, and
origin.

iii\. What does the coefficient for the year variable suggest?

For each unit increase in year, the mpg is expected to increase by 0.75 units,
holding all other variables constant.

(d) Use the plot() function to produce diagnostic plots of the linear regression
    fit. Comment on any problems you see with the fit. Do the residual plots
    suggest any unusually large outliers? Does the leverage plot identify any
    observations with unusually high leverage?

    ```{r}
    plot(mlr)
    ```

    Looking at the "Residuals vs Fitted" as well as the "Scale-Location" graphs,
    the residuals seem to be slightly cone-shaped (larger residuals with larger
    fitted values), indicating possible heteroscedasticity. R highlights
    observations 323, 326, and 327 as potential outliers as they have unusually
    high residual, which requires further investigation.

    The "Q-Q Residuals" plot validates the normality assumption as the residual
    generally follows a normal distribution (thought right skewed at the tail)
    except for very large observations such as 323, 326, and 327, which are
    potential outliers.

    In the "Residuals vs Leverage" plot, R identifies observations 14, 327, and
    394 as influential points. Among them, observation 14 has the highest
    leverage (around 0.2).

(e) Use the \* and : symbols to fit linear regression models with interaction
    effects. Do any interactions appear to be statistically significant?

    ```{r}
    mlr2 <- lm(mpg ~ (. - name)^2, data = Auto)
    summary(mlr2)
    ```

    In this model including all main effects and all two-way interactions, these
    interaction terms are statistically significant (p-value \< 0.05):
    displacement:year, acceleration:year, and acceleration:origin.

(f) Try a few different transformations of the variables, such as log(X),
    sqrt(X), X\^2. Comment on your findings.

    ```{r}
    # log(weight)
    mlr3 <- lm(mpg ~ . - name - weight + log(weight), data = Auto)
    summary(mlr3)
    ```

    This model including all variables except name but with log(weight) has a
    slightly higher adjusted R-squared than the original mlr model (0.8431 \>
    0.8182), meaning more variability is explained by the predictors.

    ```{r}
    # sqrt(horsepower)
    mlr4 <- lm(mpg ~ . - name - horsepower + sqrt(horsepower), data = Auto)
    summary(mlr4)
    ```

    This model including all variables except name but with sqrt(horse) has a
    slightly higher adjusted R-squared than the original mlr model (0.8237 \>
    0.8182), meaning more variability is explained by the predictors.

# Question 6

This problem focuses on the collinearity problem.

(a) Perform the following commands in R: \> set .seed (1) \> x1=runif (100) \>
    x2 =0.5\* x1+rnorm (100) /10 \> y=2+2\* x1 +0.3\* x2+rnorm (100) The last
    line corresponds to creating a linear model in which y is a function of x1
    and x2. Write out the form of the linear model. What are the regression
    coefficients?

```{r}
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100) / 10
y = 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

The linear model is

$$ Y = 2 + 2X_1 + 0.3X_2 + \epsilon $$

where $$\epsilon_i \sim i.i.d. N(0, 100) $$.

The coefficients are $$\beta_0 = 2$$, $$\beta_1 = 2$$, and $$\beta_2 = 0.3 $$.

(b) What is the correlation between x1 and x2? Create a scatterplot displaying
    the relationship between the variables.

```{r}
cor(x1,x2)
```

```{r}
plot(x1,x2)
```

The correlation between x1 and x2 is 0.835, which is quite high. In addition,
from the scatter plot we can learn that there is a strong positive linear
relationship b/w x1 and x2.

(c) Using this data, fit a least squares regression to predict y using x1 and
    x2. Describe the results obtained. What are ˆ B0, ˆ B1, and ˆ B2? How do
    these relate to the true B0, B1, and B2? Can you reject the null hypothesis
    H0 : B1 = 0? How about the null hypothesis H0 : B2 = 0?

```{r}
lsr <- lm(y ~ x1 + x2)
summary(lsr)
```

The fitted linear model is

$$
\hat{Y} = 2.13 + 1.44X_1 + 1.01X_2
$$

where $$\hat{\beta_0} = 2.13$$, $$\hat{\beta_1} = 1.44$$, and
$$\hat{\beta_2} = 1.01$$.

Comparing these to the true values, $$\beta_0$$ is off by 2 - 2.13 = -0.13,
$$\beta_1$$ is off by 2 - 1.44 = 0.56, and $$\beta_2$$ is off by 0.3 - 1.01 =
-0.71. This indicates a moderate bias b/w the estimated and the true parameters.

We reject the null hypothesis that $$\beta_1 = 0$$ because p-value = 0.0487 \<
0.05 threshold.

We fail to reject the null hypothesis that $$\beta_2 = 0$$ because p-value =
0.3754 \> 0.05 threshold.

(d) Now fit a least squares regression to predict y using only x1. Comment on
    your results. Can you reject the null hypothesis H0 : B1 = 0?

```{r}
lsr2 <- lm(y ~ x1)
summary(lsr2)
```

The fitted linear model is

$$
\hat{Y} = 2.11 + 1.98X_1
$$

where $$\hat{\beta_0} = 2.11$$, and $$\hat{\beta_1} = 1.98$$.

Both $$\hat{\beta_0}$$ and $$\hat{\beta_1}$$ are closer to the true values than
the previous lsr model.

We reject the null hypothesis that $$\beta_1 = 0$$ because p-value = 2.66e-06 \<
0.05 threshold.

(e) Now fit a least squares regression to predict y using only x2. Comment on
    your results. Can you reject the null hypothesis H0 : B1 = 0?

```{r}
lsr3 <- lm(y ~ x2)
summary(lsr3)
```

The fitted linear model is

$$
\hat{Y} = 2.39 + 2.90X_2
$$

where $$\hat{\beta_0} = 2.39$$, and $$\hat{\beta_1} = 2.90$$.

Both $$\hat{\beta_0}$$ and $$\hat{\beta_1}$$ are farther from the true values
than the previous lsr model.

We reject the null hypothesis that $$\beta_1 = 0$$ because p-value = 1.37e-05 \<
0.05 threshold.

(f) Do the results obtained in (c)--(e) contradict each other? Explain your
    answer.

We know that x1 and x2 has a high correlation = 0.835. As a result, if we use
both of them as predictors in the regression model, only one is statistically
significant due to the high collinearity, which is the case in (c). However,
when we split them into separate models, they each have strong relationship (as
indicated by the near-zero p-values) with the response variable Y since the
problem of collinearity is avoided. Therefore, the results from (d) and (e) both
make sense as well.
