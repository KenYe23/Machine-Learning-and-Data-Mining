---
title: "Lab 08 - Trees"
author: "Ken Ye"
date: "10/24/2019"
output: pdf_document
---

Note: some of the results for this lab depend on your version of `R` and the
version of the packages that are installed on your computer. My results differ
from the ones already in the textbook. The interpretations after printing
numerical results are meant as general trends, so don't worry if specific
numbers don't match exactly.

## 1. Classification Trees

The `tree` library is used to construct both regression and classification
trees.

```{r}
#install.packages("tree")  ## might need to update R to use
#install.packages("gbm")
```

```{r}
library(tree)
```

We will first use classification trees to analyze the `Carseats` data set. In
this data, `Sales` is a continuous variable and we begin by first recoding it as
a binary variable, using the `ifelse()` function. We will create a new variable
called `High` that will take on a value of `Yes` if `Sales` \> 8 and will take
on a value of `No` otherwise.

```{r}
library(ISLR)
attach(Carseats)
High <- ifelse(Sales > 8, "Yes", "No")

```

We can then use the `data.frame()` function to merge `High` with the rest of the
`Carseats` data.

```{r}
Carseats <- data.frame(Carseats, High)
Carseats$High = as.factor(Carseats$High)
```

Now, we can use the `tree()` function to fit a classification tree in order to
predict `High` using all variables but `Sales`. The `tree()` function has syntax
that is quite similar to the syntax of the `lm()` function.

```{r}
tree.carseats <- tree(High ~. -Sales, Carseats)
# Fit the model on all variables except for Sales
```

The `summary()` function can again be used to list the variables that are used
as internal nodes in the tree, the number of terminal nodes and the (training)
error rate.

```{r}
class(Carseats$High)
```

```{r}
summary(tree.carseats)
```

The training error is around 9%. For classification trees, the deviance reported
in the output of `summary()` is given by:

$$-2\sum_m\sum_k n_{mk}\log\hat{p}_{mk},$$

where $n_{mk}$ is the number of observations in the $m^{th}$ terminal node that
belong to the $k^{th}$ class. A small deviance indicates a tree that provides a
good fit to the (training) data. The *residual mean deviance* reported is simply
the deviance divded by $n - |T_0|$, which in this case is $400 - 27 = 373$.

One of the most attractive properties of trees is that they can be graphically
displayed. We use the `plot()` function to display the tree structure, and the
`text()` function to display the node labels. The argument `pretty = 0`
instructs `R` to include the category names for any qualitative predictors,
rather than simply displaying a letter for each category.

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

The most important predictor of `Sales` appears to be shelving location, since
the first branch differentiates `Good` locations from `Bad` and `Medium`
locations.

If we just type the name of the tree object, `R` prints output corresponding to
each branch of the tree. `R` displays the split criterior (e.g. `Price < 92.5`),
the number of observations in that branch, the deviance, the overall prediction
for the branch (`Yes` or `No`), and the fraction of observations in that branch
that take on values of `Yes` and `No`. Branches that lead to terminal nodes are
indicated using asterisks.

```{r}
tree.carseats
```

In order to properly evaluate the performance of a classification tree on this
data, we must estimate the test error rather than just the training error. We
can split the observations into a training set and a test set, build the tree
using the training set, then evaluate its performance on the test data. The
`predict()` function can be used for this purpose. In the case of a
classification tree, the argument `type = "class"` instructs `R` to return the
actual class prediction. This approach leads to correct predictions for around
71.5% of the locations of the test data set.

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200) ## split data into train and test
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
sum(diag(table(tree.pred, High.test)))/200
```

Next we consider whether pruning the tree might lead to improved results. The
function `cv.tree()` performs cross-validation in order to determine the optimal
level of tree complexity; cost complexity pruning is used in order to select a
sequence of trees for consideration. We use the argument `FUN=prune.misclass` in
order to indicate that we want the classification error rate to guide the
cross-validation and pruning process, rather than the default for the
`cv.tree()` function, which is deviance. The `cv.tree()` function reports the
number of terminal nodes of each tree considered (`size`) as well as the
corresponding error rate and the value of the cost-complexity parameter used
(`k`, which corresponds to $\alpha$ in the equation below:)

$$\sum_{m = 1}^{|T|}\sum_{x_i\in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|.$$

```{r}
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```

Note that, despite the name, `dev` corresponds to the cross-validation error
rate in this instance. The tree with 9 terminal nodes results in the lowest
cross-validation error rate, with 50 cross-validation errors. We can plot the
error rate as a function of both `size` and `k`.

```{r}
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

We now apply the `prune.misclass()` function in order to prune the tree to
obtain the nine-node tree.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

How well does this pruned tree perform on the test data set? Once again, we
apply the `predict()` function.

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
sum(diag(table(tree.pred, High.test)))/200
```

Now, 77% of the test observations are correctly classified, so not only has the
pruning process produced a more interpretable tree, but it has also improved the
classification accuracy (slightly).

If we increase the value of `best`, we obtain a larger pruned tree with lower
classification accuracy:

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
sum(diag(table(tree.pred, High.test)))/200
```

## 2. Regression Trees

Here we fit a regression tree to the `Boston` data set. First, we create a
training set and fit the tree to the training data.

```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2) # 50% split
tree.boston <- tree(medv ~., Boston, subset = train)
summary(tree.boston)
```

Notice that the output of `summary()` indicates that only three of the variables
have been used in constructing the tree. In the context of a regression tree,
the deviance is simply the sum of squared errors for the tree. We now plot the
tree:

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

We now use the `cv.tree()` function to see whether pruning the tree will improve
performance.

```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

In this case, the most complex tree is selected by cross-validation. However, if
we wish to prune the tree, we could do so as follows, using the `prune.tree()`
function:

```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

In keeping with the cross-validation results, we use the unpruned tree to make
predictions on the test set.

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```

In other words, the test set MSE associated with the regression tree is 25.05.
The square root of the MSE is therefore around 5.005, indicating that this model
leads to test predictions that are within around \$5,005 of the true median home
value for the suburb.

## 3. Bagging and Random Forests

Here we apply bagging and random forests to the `Boston` data, using the
`randomForest` package in `R`. The exact results obtained in this section may
depend on the version of `R` and the version of `randomForest` installed on your
computer.

Recall that bagging is simply a special case of random forest with $m = p$.
Therefore, the `randomForest()` function can be used to perform both random
forests and bagging. We perform bagging as follows:

```{r}
library(randomForest)
set.seed(1)
bag.Boston <- randomForest(medv~., data = Boston, subset = train,
                           mtry = 13, importance = TRUE)
bag.Boston

```

The argument `mtry = 13` indicates that all 13 predictors should be considered
for each split of the tree - in other words, that bagging should be done. How
well does this bagged model perform on the test set?

```{r}
yhat.bag <- predict(bag.Boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)^2)
```

The test set MSE associated with the bagged regression tree is 13.16, almost
half that obtained by an optimally-pruned single tree. We could change the
number of trees grown by `randomForest()` using the `ntree` argument.

```{r}
bag.Boston <- randomForest(medv~., data = Boston, subset = train,
                           mtry = 13, ntree = 25)
yhat.bag <- predict(bag.Boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
```

Growing a random forest proceeds in exactly the same way, except that we use a
smaller value of the `mtry` argument. By default, `randomForest()` uses $p/3$
variables when building a random forest of regression trees and $\sqrt{p}$
variables when building a random forest of classification trees. Here we use
`mtry = 6`.

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data = Boston, subset = train, 
                          mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf - boston.test)^2)
```

The test set MSE is 11.31; this indicates that random forests yielded an
improvement over bagging in this case.

Using the `importance()` function, we can view the importance of each variable.

```{r}
importance(rf.boston)
```

Two measures of variable importance are reported. The former is based on the
mean decrease in accuracy in predictions on the out of bag samples when a given
variable is excluded from the model. The latter is a measure of the total
decrease in node impurity that results from splits over that variable, averaged
over all trees (this was plotted in Figure 8.9 in the text). In the case of
regression trees, the node impurity is measured by the training RSS and for
classification trees by the deviance. Plots of these importance measures can be
produced using the `varImpPlot()` function.

```{r}
varImpPlot(rf.boston)
```

The results indicate that across all of the trees considered in the random
forest, the wealth level of the community (`lstat`) and the house size (`rm`)
are by far the two most important variables.

## 4. Boosting

Here we use the `gbm()` package, and within it the `gbm()` function, to fit
boosted regression trees to the `Boston` data set. We run `gbm()` with the
option `distribution = "gaussian"` since this is a regression problem; if it
were a binary classification problem, we would use `distribution = "bernoulli"`.
The argument `n.trees = 5000` indicates that we want 5000 trees, and the option
`interaction.depth = 4` limits the depth of each tree.

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~., data = Boston[train,], 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
```

The `summary()` function also provides a relative influence plot and also
outputs the relative influence statistics.

```{r}
summary(boost.boston)
```

We see that `lstat` and `rm` are by far the most important variables. We can
also produce *partial dependence plots* for these two variables. These plots
illustrate the marginal effect of the selected variables on the response after
`integrating` out the other variables. In this case, as we might expect, median
house prices are increasing with `rm` and decreasing with `lstat`.

```{r}
par(mfrow = c(1,2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

We now use the boosted model to predict `medv` on the test set:

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,],
                      n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

The test MSE obtained is 11.8; similar to the test MSE for random forests and
superior to that for bagging. If we want to, we can perform boosting with a
different value of the shrinkage parameter $\lambda$ in Equation 8.10. The
default value is 0.001, but this is easily modified. Here, we take
$\lambda = 0.2$.

## 5. Problems

For these problems, we will work with the `Carseats` data again, this time in a
regression setting, where the goal is to predict `Sales`. Make sure to drop the
`High` variable in every model.

### 1. Train/Test Split

Split the `Carseats` data into a training and test set, using 30% of the data
for the test set.

### 2. Regression Tree

Fit a regular regression tree on the training data using cross validation.
Decide at what level to prune the tree and how you decided this. Report the test
MSE for your tree and plot your final tree.

### 3. Bagging

Perform bagging for the `Carseats` data with 25 trees. Report the MSE on the
test set.

### 4. Random Forest

Now, fit a random forest to the `Carseats` data. Report the variable importance
as a plot and the MSE on the test set. Use $m = 3$.

### 5. Boosting

Finally, perform boosting on the `Carseats` data. Again, report the MSE on the
test set. Use an interaction depth of 3.

### 6. Model Selection

Make a table/dataframe summarizing the MSE results for each model considered
above. Which model would you select and why? Which variables appear important to
the trees? Does this make sense in the context of the problem?
