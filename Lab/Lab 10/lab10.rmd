---
title: "Lab 10 - Dimensionality Reduction and Evaluation Metrics"
author: "Ken Ye"
date: "11/8/2023"
output: pdf_document
---

## 1. Principal Components Analysis (PCA)

In this lab, we perform PCA on the `USArrests` data set, which is part of the
base `R` package. The rows of the data set contain the 50 states, in
alphabetical order.

```{r}
states <- row.names(USArrests)
states
```

The columns of the data set contain the four variables:

```{r}
names(USArrests)
```

We first briefly examing the data. We notice that the variables have vastly
different means:

```{r}
apply(USArrests, 2, mean)
```

Note that the `apply()` function allows us to apply a function - in this case,
the `mean()` function - to each row or column of the data set. The second input
here denotes whether we wish to commute the mean of the rows, `1`, or the
columns, `2`. We see that there are many more assualts than other types of
violent crimes. We can also examine the variances of the four variables, using
the `apply()` function.

```{r}
apply(USArrests, 2, var)
```

Not surprisingly, the variables also have vastly different variances. The
`UrbanPop` variable measures the percentage of the population in each state
living in an urban area, which is not a comparable number to the number of
violent crimes in each state of each type per 100,000 individuals.

If we failed to scale the variables before performing PCA, then most of the
principal components that we observed would be driven by the `Assault` variable,
since it has by far the largest mean and variance. Thus, it is important to
standardize the variables to have mean zero and standard deviation one before
performing PCA.

We now perform principal components analysis using the `prcomp()` function,
which is one of several functions in `R` that perform PCA.

```{r}
pr.out <- prcomp(USArrests, scale = TRUE)
```

By default, the `prcomp()` function centers the variables to have mean zero. By
using the option `scale = TRUE`, we scale the variables to have standard
deviation one. The output from `prcomp()` contains a number of useful
quantities.

```{r}
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA.

```{r}
pr.out$center
pr.out$scale
```

The `rotation` matrix provides the principal component loadings; each column of
`pr.out$rotation` contains the corresponding principal component loading vector.

```{r}
pr.out$rotation
```

We see that there are four distinct principal components. This is to be expected
because there are in general $\min (n-1,p)$ informative principal components in
a data set with $n$ observations and $p$ variables.

Using the `prcomp()` function, we do not need to explicitly multiply the data by
the principal component loading vectors in order to obtain the principal
component score vectors. Rather the 50x4 matrix $x$ has as its columns the
principal component score vectors. That is, the $k$th column is the $k$th
principal component score vector.

```{r}
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument to `biplot()` ensures that the arrows are scaled to
represent the loadings; other values for `scale` give slightly different biplots
with different interpretations.

Notice that this figure is a mirror image of Figure 10.1. Recall that the
principal components are only unique up to a sign change, so we can reproduce
Figure 10.1 by making a few small changes:

```{r}
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0)
```

The `prcomp()` function also outputs the standard deviation of each principal
component. For instance, on the `USArrests` data set, we can access these
standard deviations as follows:

```{r}
pr.out$sdev
```

The variance explained by each principal component is obtained by squaring
these:

```{r}
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal component, we
simply divide the variance explained by each principal component by the total
variance explained by all four principal components:

```{r}
pve <- pr.var/sum(pr.var)
pve
```

We see that the first principal component explains 62.0% of the variance in the
data, the next principal component explains 24.7% of the variance, and so forth.
We can plot the PVE explained by each component, as well as the cumulative PVE,
as follows:

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", ylim = c(0,1),
     type = 'b')
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cummulative Proportion of Variance Explained",
     ylim = c(0,1), type = 'b')
```

The result is shown in Figure 10.4. Note that the function `cumsum()` computes
the cumulative sum of the elements of a numeric vector. For instance,

```{r}
a <- c(1,2,8,-3)
cumsum(a)
```

There are many different ways to calculate and display PCA in `R`. For example,
a `ggplot` version can be plotted using the `ggfortify` package. This package
also allows for several other types of commonly used plots beyond the
functionality provided in regular `ggplot`, check out
<http://www.sthda.com/english/wiki/ggfortify-extension-to-ggplot2-to-handle-some-popular-packages-r-software-and-data-visualization>.

```{r, warning = FALSE, message = FALSE}
library(ggfortify)
autoplot(pr.out, loadings = TRUE, loadings.label = TRUE,
         data = USArrests)
```

## 2. tSNE

Another commonly used dimensionality reduction technique is called t-Distributed
Stochastic Neighbor Embedding. We can also run this in `R` on the `iris`
dataset.

```{r, warning = FALSE, message = FALSE}
library(Rtsne)
library(ggplot2)
data(iris)
```

```{r}

tsne <- Rtsne(iris[,-5], dims = 2, perplexity=30,
              verbose=TRUE,max_iter = 500,
              check_duplicates = FALSE)

embedding <- data.frame(tsne$Y)
embedding$Species <- iris$Species

ggplot(embedding, aes(x = X1, y = X2, color = Species)) +
  geom_point()

```

We can see that the `iris` data separates well by species when we take into
account all 4 features and perform a tSNE dimensionality reduction. A linear
classifier would do quite well on this transformed data.

## 3. Evaluation Metrics

### 3.1 Classification

Write a function that calculates the precision, recall and accuracy of a
classifier and also outputs the confusion matrix. Assume that the inputs to the
function are the true labels and the predicted labels for the same input data.

Fit a classifier of your choice on the iris data and test your function (only
use the species versicolor and virginica for the classifer so there are only 2
classes). Build a different model and compare the two results using your
evaluation function. Which model would you select and why based on the outputs
of your function?

```{r}
class.eval <- function(true.labels, pred.labels){
  
  
  
}
```

```{r}
true.labs <- c(0,1,1,0,1,0,1)
pred.labs <- c(0,0,1,1,1,1,0)
class.eval(true.labs, pred.labs)
```

### 3.2 Implementation

Give an example of a case where false positives are much worse than false
negatives. When is a case where false negatives could be much worse than false
positives?

### 3.3 Regression

Write a function that calculates the MSE, RMSE and mean absolute error for a
regression function. The inputs to your function should be the true $Y_i$ values
and the predicted $\hat{Y}_i$ values.

Use the `mtcars` data to test your function. Build a regression model of your
choice to test your function. Assume a 70%-30% training-test split and evaluate
your model on the test set. The response variable is `mpg`.

```{r}
data(mtcars)
head(mtcars)
```

```{r}
regress.eval <- function(true.vals, pred.vals){
  
  
  
}
```

```{r}
toy.true <- c(1,2,3,4,5)
toy.pred <- c(2,1,3.1,5.4, 9.0)
regress.eval(toy.true, toy.pred)
```
