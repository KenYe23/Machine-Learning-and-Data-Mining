---
title: "Lab 10 - Dimensionality Reduction and Evaluation Metrics"
author: "Ken Ye"
date: "11/8/2023"
output: pdf_document
---

## 1. Principal Components Analysis (PCA)

In this lab, we perform PCA on the `USArrests` data set, which is part
of the base `R` package. The rows of the data set contain the 50 states,
in alphabetical order.

```{r}
states <- row.names(USArrests)
states
```

The columns of the data set contain the four variables:

```{r}
names(USArrests)
```

We first briefly examine the data. We notice that the variables have
vastly different means:

```{r}
apply(USArrests, 2, mean)
```

Note that the `apply()` function allows us to apply a function - in this
case, the `mean()` function - to each row or column of the data set. The
second input here denotes whether we wish to commute the mean of the
rows, `1`, or the columns, `2`. We see that there are many more assaults
than other types of violent crimes. We can also examine the variances of
the four variables, using the `apply()` function.

```{r}
apply(USArrests, 2, var)
```

Not surprisingly, the variables also have vastly different variances.
The `UrbanPop` variable measures the percentage of the population in
each state living in an urban area, which is not a comparable number to
the number of violent crimes in each state of each type per 100,000
individuals.

If we failed to scale the variables before performing PCA, then most of
the principal components that we observed would be driven by the
`Assault` variable, since it has by far the largest mean and variance.
Thus, it is important to standardize the variables to have mean zero and
standard deviation one before performing PCA.

We now perform principal components analysis using the `prcomp()`
function, which is one of several functions in `R` that perform PCA.

```{r}
pr.out <- prcomp(USArrests, scale = TRUE)
```

By default, the `prcomp()` function centers the variables to have mean
zero. By using the option `scale = TRUE`, we scale the variables to have
standard deviation one. The output from `prcomp()` contains a number of
useful quantities.

```{r}
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard
deviations of the variables that were used for scaling prior to
implementing PCA.

```{r}
pr.out$center
pr.out$scale
```

The `rotation` matrix provides the principal component loadings; each
column of `pr.out$rotation` contains the corresponding principal
component loading vector.

```{r}
pr.out$rotation
```

We see that there are four distinct principal components. This is to be
expected because there are in general $\min (n-1,p)$ informative
principal components in a data set with $n$ observations and $p$
variables.

Using the `prcomp()` function, we do not need to explicitly multiply the
data by the principal component loading vectors in order to obtain the
principal component score vectors. Rather the 50x4 matrix $x$ has as its
columns the principal component score vectors. That is, the $k$th column
is the $k$th principal component score vector.

```{r}
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument to `biplot()` ensures that the arrows are
scaled to represent the loadings; other values for `scale` give slightly
different biplots with different interpretations.

Notice that this figure is a mirror image of Figure 10.1. Recall that
the principal components are only unique up to a sign change, so we can
reproduce Figure 10.1 by making a few small changes:

```{r}
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0)
```

The `prcomp()` function also outputs the standard deviation of each
principal component. For instance, on the `USArrests` data set, we can
access these standard deviations as follows:

```{r}
pr.out$sdev
```

The variance explained by each principal component is obtained by
squaring these:

```{r}
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal
component, we simply divide the variance explained by each principal
component by the total variance explained by all four principal
components:

```{r}
pve <- pr.var/sum(pr.var)
pve
```

We see that the first principal component explains 62.0% of the variance
in the data, the next principal component explains 24.7% of the
variance, and so forth. We can plot the PVE explained by each component,
as well as the cumulative PVE, as follows:

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", ylim = c(0,1),
     type = 'b')
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cummulative Proportion of Variance Explained",
     ylim = c(0,1), type = 'b')
```

The result is shown in Figure 10.4. Note that the function `cumsum()`
computes the cumulative sum of the elements of a numeric vector. For
instance,

```{r}
a <- c(1,2,8,-3)
cumsum(a)
```

There are many different ways to calculate and display PCA in `R`. For
example, a `ggplot` version can be plotted using the `ggfortify`
package. This package also allows for several other types of commonly
used plots beyond the functionality provided in regular `ggplot`, check
out
<http://www.sthda.com/english/wiki/ggfortify-extension-to-ggplot2-to-handle-some-popular-packages-r-software-and-data-visualization>.

```{r, warning = FALSE, message = FALSE}
library(ggfortify)
autoplot(pr.out, loadings = TRUE, loadings.label = TRUE,
         data = USArrests)
```

## 2. tSNE

Another commonly used dimensionality reduction technique is called
t-Distributed Stochastic Neighbor Embedding. We can also run this in `R`
on the `iris` dataset.

```{r, warning = FALSE, message = FALSE}
library(Rtsne)
library(ggplot2)
data(iris)
```

```{r}
tsne <- Rtsne(iris[,-5], dims = 2, perplexity=30,
              verbose=TRUE,max_iter = 500,
              check_duplicates = FALSE)

embedding <- data.frame(tsne$Y)
embedding$Species <- iris$Species

ggplot(embedding, aes(x = X1, y = X2, color = Species)) +
  geom_point()

```

We can see that the `iris` data separates well by species when we take
into account all 4 features and perform a tSNE dimensionality reduction.
A linear classifier would do quite well on this transformed data.

## 3. Evaluation Metrics

### 3.1 Classification

Write a function that calculates the precision, recall and accuracy of a
classifier and also outputs the confusion matrix. Assume that the inputs
to the function are the true labels and the predicted labels for the
same input data.

Fit a classifier of your choice on the iris data and test your function
(only use the species versicolor and virginica for the classifer so
there are only 2 classes). Build a different model and compare the two
results using your evaluation function. Which model would you select and
why based on the outputs of your function?

```{r}
class.eval <- function(true.labels, pred.labels) {
  # Calculate confusion matrix
  conf_matrix <- table(true.labels, pred.labels)
  
  # Calculate precision, recall, and accuracy
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
  recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  
  # Print confusion matrix and evaluation metrics
  cat("Confusion Matrix:\n")
  print(conf_matrix)
  
  cat("\nPrecision:", precision, "\n")
  cat("Recall:", recall, "\n")
  cat("Accuracy:", accuracy, "\n")
}
```

```{r}
true.labs <- c(0,1,1,0,1,0,1)
pred.labs <- c(0,0,1,1,1,1,0)
class.eval(true.labs, pred.labs)
```

```{r}
# Filter the dataset to include only 'versicolor' and 'virginica' classes
iris_subset <- subset(iris, Species %in% c("versicolor", "virginica"))

# Remove 'setosa' level from the 'Species' variable
iris_subset$Species <- droplevels(iris_subset$Species, exclude = "setosa")
```

```{r}
# Fit Logistic Regression model
model_logreg <- glm(Species ~ ., data = iris_subset, family = "binomial")
predictions_logreg <- predict(model_logreg, newdata = iris_subset, type = "response") > 0.5
predictions_logreg <- as.factor(ifelse(predictions_logreg, "virginica", "versicolor"))

# Evaluate Logistic Regression model
class.eval(iris_subset$Species, predictions_logreg)
```

```{r}
library(rpart)
# Fit Decision Tree model
model_dt <- rpart(Species ~ ., data = iris_subset, method = "class")
predictions_dt <- predict(model_dt, newdata = iris_subset, type = "class")

class.eval(iris_subset$Species, predictions_dt)
```

Comparing the logistic regression model and the decision tree model, I
would choose the logistic regression model because its precision,
recall, and accuracy values are all higher.

### 3.2 Implementation

Give an example of a case where false positives are much worse than
false negatives. When is a case where false negatives could be much
worse than false positives?

-   In the spam email filtering scenario, false positives can
    inconvenience users by filtering out legitimate emails, but it is
    usually not life-threatening.

-   In the medical testing scenario, false negatives can have serious
    consequences, as failing to identify a severe disease can delay
    treatment and harm the patient.

### 3.3 Regression

Write a function that calculates the MSE, RMSE and mean absolute error
for a regression function. The inputs to your function should be the
true $Y_i$ values and the predicted $\hat{Y}_i$ values.

Use the `mtcars` data to test your function. Build a regression model of
your choice to test your function. Assume a 70%-30% training-test split
and evaluate your model on the test set. The response variable is `mpg`.

```{r}
data(mtcars)
head(mtcars)
```

```{r}
regress.eval <- function(true.vals, pred.vals) {
  # Calculate Mean Squared Error (MSE)
  mse <- mean((true.vals - pred.vals)^2)
  
  # Calculate Root Mean Squared Error (RMSE)
  rmse <- sqrt(mse)
  
  # Calculate Mean Absolute Error (MAE)
  mae <- mean(abs(true.vals - pred.vals))
  
  # Print evaluation metrics
  cat("Mean Squared Error (MSE):", mse, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse, "\n")
  cat("Mean Absolute Error (MAE):", mae, "\n")
}
```

```{r}
toy.true <- c(1,2,3,4,5)
toy.pred <- c(2,1,3.1,5.4, 9.0)
regress.eval(toy.true, toy.pred)
```

```{r}
set.seed(1)
train_indices <- sample(1:nrow(mtcars), 0.7 * nrow(mtcars))
train_data <- mtcars[train_indices, ]
test_data <- mtcars[-train_indices, ]
```

```{r}
# Fit a linear regression model
model <- lm(mpg ~ ., data = train_data)

# Make predictions on the test set
predictions <- predict(model, newdata = test_data)

# Evaluate the regression model using the custom function
regress.eval(test_data$mpg, predictions)
```
